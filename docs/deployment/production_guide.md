# Bento Framework Outbox ç”Ÿäº§éƒ¨ç½²æŒ‡å—

## ğŸš€ æ¦‚è¿°

æœ¬æŒ‡å—æä¾›äº†Bento Framework Outboxå’ŒMessageBusç³»ç»Ÿåœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„å®Œæ•´éƒ¨ç½²æ–¹æ¡ˆï¼ŒåŒ…æ‹¬æ€§èƒ½ä¼˜åŒ–ã€å®‰å…¨é…ç½®ã€ç›‘æ§è®¾ç½®å’Œæœ€ä½³å®è·µã€‚

## ğŸ“‹ éƒ¨ç½²æ£€æŸ¥æ¸…å•

### âœ… ç¯å¢ƒå‡†å¤‡
- [ ] PostgreSQL 12+ æ•°æ®åº“
- [ ] Python 3.11+ è¿è¡Œç¯å¢ƒ
- [ ] Redis/Pulsaræ¶ˆæ¯é˜Ÿåˆ—
- [ ] ç›‘æ§ç³»ç»Ÿ (å¯é€‰)

### âœ… é…ç½®ä¼˜åŒ–
- [ ] æ•°æ®åº“è¿æ¥æ± é…ç½®
- [ ] OutboxProjectoræ€§èƒ½å‚æ•°
- [ ] ç´¢å¼•åˆ›å»ºå’Œä¼˜åŒ–
- [ ] ç¯å¢ƒå˜é‡è®¾ç½®

### âœ… å®‰å…¨é…ç½®
- [ ] æ•°æ®åº“è¿æ¥å®‰å…¨
- [ ] äº‹ä»¶åºåˆ—åŒ–å®‰å…¨
- [ ] ç½‘ç»œå®‰å…¨é…ç½®

### âœ… ç›‘æ§è®¾ç½®
- [ ] æ€§èƒ½æŒ‡æ ‡ç›‘æ§
- [ ] å‘Šè­¦é˜ˆå€¼è®¾ç½®
- [ ] æ—¥å¿—é…ç½®

## ğŸ—„ï¸ æ•°æ®åº“é…ç½®

### PostgreSQLç”Ÿäº§é…ç½®

```sql
-- 1. åˆ›å»ºOutboxè¡¨ (å·²åŒ…å«ä¼˜åŒ–ç´¢å¼•)
CREATE TABLE outbox (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant_id VARCHAR(128) NOT NULL,
    aggregate_id VARCHAR(128),
    aggregate_type VARCHAR(100),
    topic VARCHAR(128) NOT NULL,
    occurred_at TIMESTAMP WITH TIME ZONE NOT NULL,
    schema_id VARCHAR(128),
    schema_version INTEGER DEFAULT 1,
    payload JSONB NOT NULL,
    event_metadata JSONB DEFAULT '{}',
    status VARCHAR(10) DEFAULT 'NEW',
    retry_count INTEGER DEFAULT 0,
    retry_after TIMESTAMP WITH TIME ZONE,
    error_message VARCHAR(500),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- 2. æ€§èƒ½ä¼˜åŒ–ç´¢å¼•
CREATE INDEX ix_tenant_id_status ON outbox(tenant_id, status);
CREATE INDEX ix_outbox_processing ON outbox(status, retry_after);
CREATE INDEX ix_outbox_topic ON outbox(topic);
CREATE INDEX ix_outbox_aggregate ON outbox(aggregate_type, aggregate_id);

-- P2-B æ€§èƒ½ä¼˜åŒ–ç´¢å¼•
CREATE INDEX ix_outbox_cleanup ON outbox(tenant_id, created_at);
CREATE INDEX ix_outbox_query_opt ON outbox(status, retry_after, tenant_id);
CREATE INDEX ix_outbox_tenant_created ON outbox(tenant_id, created_at, status);
CREATE INDEX ix_outbox_processing_tenant ON outbox(tenant_id, status, retry_count);
```

### è¿æ¥æ± é…ç½®

```python
# production_db.py
from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker

def create_production_engine(database_url: str):
    """ç”Ÿäº§ç¯å¢ƒæ•°æ®åº“å¼•æ“é…ç½®"""
    return create_async_engine(
        database_url,
        # è¿æ¥æ± é…ç½®
        pool_size=20,              # åŸºç¡€è¿æ¥æ± å¤§å°
        max_overflow=10,           # å…è®¸æº¢å‡ºè¿æ¥æ•°
        pool_pre_ping=True,        # è¿æ¥éªŒè¯
        pool_recycle=3600,         # 1å°æ—¶å›æ”¶è¿æ¥

        # æ€§èƒ½é…ç½®
        echo=False,                # ç”Ÿäº§ç¯å¢ƒå…³é—­SQLæ—¥å¿—
        future=True,               # ä½¿ç”¨SQLAlchemy 2.0 API

        # è¶…æ—¶é…ç½®
        connect_args={
            "command_timeout": 60,
            "server_settings": {
                "jit": "off",  # å…³é—­JITä»¥è·å¾—ç¨³å®šæ€§èƒ½
            }
        }
    )

# ä½¿ç”¨ç¤ºä¾‹
DATABASE_URL = "postgresql+asyncpg://user:password@host:5432/database"
engine = create_production_engine(DATABASE_URL)
session_factory = async_sessionmaker(engine, expire_on_commit=False)
```

## âš™ï¸ OutboxProjectorç”Ÿäº§é…ç½®

### é«˜æ€§èƒ½é…ç½®

```python
# config/production.py
from bento.config.outbox import OutboxProjectorConfig

# é«˜ååé‡é…ç½®
HIGH_THROUGHPUT_CONFIG = OutboxProjectorConfig(
    # æ ¸å¿ƒæ€§èƒ½å‚æ•°
    batch_size=1000,                    # å¤§æ‰¹é‡å¤„ç†
    max_concurrent_projectors=8,        # å¤šå¹¶å‘å¤„ç†

    # è½®è¯¢ç­–ç•¥
    sleep_busy=0.05,                    # 5mså¿«é€Ÿè½®è¯¢
    sleep_idle=0.5,                     # 500msç©ºé—²è½®è¯¢
    sleep_idle_max=5.0,                 # æœ€å¤§5sç©ºé—²

    # é‡è¯•ç­–ç•¥
    max_retry_attempts=5,               # æœ€å¤§é‡è¯•5æ¬¡
    error_retry_delay=2.0,              # 2sé‡è¯•é—´éš”

    # æ€§èƒ½ç›‘æ§
    enable_performance_monitoring=True, # å¯ç”¨ç›‘æ§

    # æ•°æ®åº“ä¼˜åŒ–
    connection_pool_size=20,            # è¿æ¥æ± å¤§å°
    query_timeout_seconds=30,           # æŸ¥è¯¢è¶…æ—¶
    batch_commit_size=2000,             # æ‰¹é‡æäº¤

    # å¤šç§Ÿæˆ·
    default_tenant_id="production"
)

# ä½å»¶è¿Ÿé…ç½® (é€‚åˆå®æ—¶åœºæ™¯)
LOW_LATENCY_CONFIG = OutboxProjectorConfig(
    batch_size=100,                     # å°æ‰¹é‡å¿«é€Ÿå¤„ç†
    max_concurrent_projectors=15,       # é«˜å¹¶å‘
    sleep_busy=0.01,                    # 10msæé€Ÿè½®è¯¢
    sleep_idle=0.1,                     # 100mså¿«é€Ÿå“åº”
    max_retry_attempts=3,               # å¿«é€Ÿå¤±è´¥
    enable_performance_monitoring=True
)
```

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# .env.production
# æ•°æ®åº“é…ç½®
DATABASE_URL=postgresql+asyncpg://outbox_user:secure_password@db-host:5432/outbox_db

# Outboxé…ç½®
BENTO_OUTBOX_BATCH_SIZE=1000
BENTO_OUTBOX_MAX_CONCURRENT_PROJECTORS=8
BENTO_OUTBOX_SLEEP_BUSY=0.05
BENTO_OUTBOX_SLEEP_IDLE=0.5
BENTO_OUTBOX_MAX_RETRY_ATTEMPTS=5
BENTO_OUTBOX_ENABLE_PERFORMANCE_MONITORING=true

# æ¶ˆæ¯é˜Ÿåˆ—é…ç½®
PULSAR_SERVICE_URL=pulsar://pulsar-cluster:6650
REDIS_URL=redis://redis-cluster:6379

# ç›‘æ§é…ç½®
METRICS_PORT=9090
LOG_LEVEL=INFO

# å®‰å…¨é…ç½®
ENCRYPTION_KEY=your-32-byte-encryption-key
```

## ğŸ“Š æ€§èƒ½ç›‘æ§é…ç½®

### ç›‘æ§è®¾ç½®

```python
# monitoring/setup.py
from bento.infrastructure.monitoring.performance import PerformanceMonitor
import asyncio
import logging

async def setup_monitoring(session_factory, projector):
    """è®¾ç½®ç”Ÿäº§ç¯å¢ƒç›‘æ§"""

    # 1. æ€§èƒ½ç›‘æ§
    monitor = PerformanceMonitor(session_factory)

    # 2. å®šæœŸå¥åº·æ£€æŸ¥
    async def health_check():
        while True:
            try:
                # è·å–æ€§èƒ½æŒ‡æ ‡
                metrics = await monitor.get_metrics()

                # åˆ†æç“¶é¢ˆ
                analysis = await monitor.analyze_performance_bottlenecks()

                # è®°å½•å…³é”®æŒ‡æ ‡
                logging.info(
                    f"Outbox Performance - "
                    f"Pending: {metrics.pending_events}, "
                    f"EPS: {metrics.events_per_second:.2f}, "
                    f"Query Time: {metrics.avg_query_time_ms:.2f}ms"
                )

                # é«˜ä¸¥é‡æ€§å‘Šè­¦
                if analysis['severity'] in ['high', 'critical']:
                    logging.error(f"Performance Alert: {analysis['bottlenecks']}")

                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

            except Exception as e:
                logging.error(f"Health check failed: {e}")
                await asyncio.sleep(60)

    # å¯åŠ¨åå°ç›‘æ§
    asyncio.create_task(health_check())
    return monitor
```

### å‘Šè­¦é…ç½®

```python
# monitoring/alerts.py
PERFORMANCE_THRESHOLDS = {
    'pending_events_critical': 50000,      # 5ä¸‡äº‹ä»¶ç§¯å‹
    'pending_events_warning': 10000,       # 1ä¸‡äº‹ä»¶ç§¯å‹
    'avg_query_time_critical': 1000,       # 1ç§’æŸ¥è¯¢æ—¶é—´
    'avg_query_time_warning': 500,         # 500msæŸ¥è¯¢æ—¶é—´
    'events_per_second_min': 100,          # æœ€å°å¤„ç†é€Ÿç‡
    'connection_pool_usage_critical': 0.9, # 90%è¿æ¥æ± ä½¿ç”¨ç‡
}

async def check_alerts(metrics):
    """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
    alerts = []

    if metrics.pending_events > PERFORMANCE_THRESHOLDS['pending_events_critical']:
        alerts.append({
            'level': 'CRITICAL',
            'message': f'Large event backlog: {metrics.pending_events} events',
            'action': 'Increase projector instances or batch size'
        })

    if metrics.avg_query_time_ms > PERFORMANCE_THRESHOLDS['avg_query_time_critical']:
        alerts.append({
            'level': 'CRITICAL',
            'message': f'Slow queries: {metrics.avg_query_time_ms:.2f}ms average',
            'action': 'Check database performance and indexes'
        })

    return alerts
```

## ğŸ§¹ è¿ç»´ä»»åŠ¡

### å†å²æ•°æ®æ¸…ç†

```python
# maintenance/cleanup.py
from bento.infrastructure.monitoring.performance import cleanup_old_outbox_records
import asyncio

async def schedule_cleanup(session_factory):
    """å®šæœŸæ¸…ç†å†å²æ•°æ®"""

    while True:
        try:
            # ä¿ç•™7å¤©æ•°æ®ï¼Œæ¯å¤©æ¸…ç†ä¸€æ¬¡
            stats = await cleanup_old_outbox_records(
                session_factory,
                retention_days=7,
                batch_size=5000,
                dry_run=False  # å®é™…æ‰§è¡Œ
            )

            logging.info(
                f"Cleanup completed - "
                f"Deleted: {stats['deleted']} records in {stats['batches']} batches"
            )

            # æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œ
            await asyncio.sleep(24 * 3600)

        except Exception as e:
            logging.error(f"Cleanup failed: {e}")
            await asyncio.sleep(3600)  # 1å°æ—¶åé‡è¯•
```

### éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# deploy.sh - ç”Ÿäº§éƒ¨ç½²è„šæœ¬

set -e

echo "ğŸš€ Deploying Bento Outbox to Production"

# 1. ç¯å¢ƒæ£€æŸ¥
echo "ğŸ“‹ Environment Check..."
python -c "import sys; assert sys.version_info >= (3, 11), 'Python 3.11+ required'"
psql $DATABASE_URL -c "SELECT version();" > /dev/null

# 2. æ•°æ®åº“è¿ç§»
echo "ğŸ—„ï¸ Database Migration..."
python -m alembic upgrade head

# 3. ç´¢å¼•æ£€æŸ¥å’Œåˆ›å»º
echo "ğŸ“Š Database Optimization..."
python scripts/create_indexes.py

# 4. é…ç½®éªŒè¯
echo "âš™ï¸ Configuration Validation..."
python -c "
from bento.config.validation import validate_config
from bento.config.outbox import get_outbox_projector_config

config = get_outbox_projector_config()
result = validate_config(config)
assert result.is_valid, f'Config validation failed: {result.errors}'
print('âœ… Configuration is valid')
"

# 5. å¯åŠ¨æœåŠ¡
echo "ğŸš€ Starting Services..."
python -m gunicorn app:app --worker-class uvicorn.workers.UvicornWorker \
    --workers 4 --bind 0.0.0.0:8000 &

python -m bento.infrastructure.projection.projector_service &

echo "âœ… Deployment completed successfully!"
```

## ğŸ”’ å®‰å…¨æœ€ä½³å®è·µ

### æ•°æ®åº“å®‰å…¨

```python
# security/database.py
import os
from cryptography.fernet import Fernet

# 1. æ•æ„Ÿæ•°æ®åŠ å¯†
def encrypt_sensitive_payload(payload: dict) -> dict:
    """åŠ å¯†æ•æ„Ÿäº‹ä»¶æ•°æ®"""
    cipher = Fernet(os.environ['ENCRYPTION_KEY'].encode())

    sensitive_fields = ['password', 'token', 'secret', 'key']
    encrypted_payload = payload.copy()

    for field in sensitive_fields:
        if field in encrypted_payload:
            encrypted_payload[field] = cipher.encrypt(
                str(encrypted_payload[field]).encode()
            ).decode()

    return encrypted_payload

# 2. è¿æ¥å®‰å…¨
DATABASE_CONFIG = {
    'sslmode': 'require',           # å¼ºåˆ¶SSL
    'sslcert': '/path/to/client.crt',
    'sslkey': '/path/to/client.key',
    'sslrootcert': '/path/to/ca.crt'
}
```

### ç½‘ç»œå®‰å…¨

```yaml
# docker-compose.production.yml
version: '3.8'
services:
  outbox-projector:
    image: bento-outbox:latest
    networks:
      - internal
    environment:
      - DATABASE_URL=postgresql://...
      - ENCRYPTION_KEY=${ENCRYPTION_KEY}
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'

networks:
  internal:
    driver: overlay
    internal: true  # å†…éƒ¨ç½‘ç»œï¼Œä¸æš´éœ²åˆ°å¤–éƒ¨
```

## ğŸ“ˆ æ€§èƒ½åŸºå‡†å’Œå®¹é‡è§„åˆ’

### æ€§èƒ½åŸºå‡†

```python
# benchmarks/performance_test.py
async def performance_benchmark():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    test_scenarios = [
        {'batch_size': 100, 'concurrent': 1, 'expected_tps': 1000},
        {'batch_size': 500, 'concurrent': 5, 'expected_tps': 5000},
        {'batch_size': 1000, 'concurrent': 10, 'expected_tps': 10000}
    ]

    for scenario in test_scenarios:
        print(f"Testing: {scenario}")
        # æ‰§è¡Œæ€§èƒ½æµ‹è¯•...
        actual_tps = await run_performance_test(scenario)

        if actual_tps >= scenario['expected_tps']:
            print(f"âœ… PASS - {actual_tps} TPS")
        else:
            print(f"âŒ FAIL - {actual_tps} TPS (expected {scenario['expected_tps']})")
```

### å®¹é‡è§„åˆ’

| åœºæ™¯ | äº‹ä»¶é‡/å¤© | æ¨èé…ç½® | èµ„æºéœ€æ±‚ |
|------|-----------|----------|----------|
| **å°å‹** | 10ä¸‡ | batch_size=200, concurrent=2 | 2 CPU, 4GB RAM |
| **ä¸­å‹** | 100ä¸‡ | batch_size=500, concurrent=5 | 4 CPU, 8GB RAM |
| **å¤§å‹** | 1000ä¸‡ | batch_size=1000, concurrent=10 | 8 CPU, 16GB RAM |
| **è¶…å¤§å‹** | 1äº¿+ | batch_size=2000, concurrent=20 | 16 CPU, 32GB RAM |

## ğŸš¨ æ•…éšœæ’æŸ¥æŒ‡å—

### å¸¸è§é—®é¢˜

1. **äº‹ä»¶ç§¯å‹**
   - æ£€æŸ¥: `SELECT COUNT(*) FROM outbox WHERE status='NEW'`
   - è§£å†³: å¢åŠ batch_sizeæˆ–concurrent_projectors

2. **æŸ¥è¯¢ç¼“æ…¢**
   - æ£€æŸ¥: ç´¢å¼•ä½¿ç”¨æƒ…å†µ
   - è§£å†³: é‡å»ºç´¢å¼•ï¼Œä¼˜åŒ–æŸ¥è¯¢

3. **è¿æ¥æ± è€—å°½**
   - æ£€æŸ¥: è¿æ¥æ± ä½¿ç”¨ç‡
   - è§£å†³: å¢åŠ pool_sizeæˆ–ä¼˜åŒ–è½®è¯¢é¢‘ç‡

### è¯Šæ–­è„šæœ¬

```python
# scripts/diagnose.py
async def diagnose_outbox_health(session_factory):
    """Outboxç³»ç»Ÿå¥åº·è¯Šæ–­"""

    print("ğŸ” Outbox Health Diagnosis")
    print("=" * 30)

    async with session_factory() as session:
        # 1. äº‹ä»¶çŠ¶æ€åˆ†å¸ƒ
        result = await session.execute(
            text("SELECT status, COUNT(*) FROM outbox GROUP BY status")
        )
        print("ğŸ“Š Event Status Distribution:")
        for row in result:
            print(f"   {row.status}: {row.count}")

        # 2. ç§¯å‹åˆ†æ
        result = await session.execute(
            text("SELECT COUNT(*) FROM outbox WHERE created_at < NOW() - INTERVAL '1 hour'")
        )
        old_events = result.scalar()
        print(f"â° Events older than 1 hour: {old_events}")

        # 3. å¤±è´¥åˆ†æ
        result = await session.execute(
            text("SELECT error_message, COUNT(*) FROM outbox WHERE status='FAILED' GROUP BY error_message LIMIT 10")
        )
        print("âŒ Top failure reasons:")
        for row in result:
            print(f"   {row.error_message}: {row.count}")
```

## ğŸ¯ æ€»ç»“

è¿™ä»½ç”Ÿäº§éƒ¨ç½²æŒ‡å—æ¶µç›–äº†Bento Framework Outboxç³»ç»Ÿåœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„å„ä¸ªæ–¹é¢ï¼š

âœ… **å®Œæ•´çš„é…ç½®æ–¹æ¡ˆ** - æ•°æ®åº“ã€è¿æ¥æ± ã€æ€§èƒ½å‚æ•°
âœ… **ç›‘æ§å’Œå‘Šè­¦** - å®æ—¶æŒ‡æ ‡ã€å¥åº·æ£€æŸ¥ã€é—®é¢˜é¢„è­¦
âœ… **å®‰å…¨æœ€ä½³å®è·µ** - æ•°æ®åŠ å¯†ã€ç½‘ç»œå®‰å…¨ã€è®¿é—®æ§åˆ¶
âœ… **è¿ç»´è‡ªåŠ¨åŒ–** - éƒ¨ç½²è„šæœ¬ã€æ¸…ç†ä»»åŠ¡ã€è¯Šæ–­å·¥å…·
âœ… **æ€§èƒ½è°ƒä¼˜** - åŸºå‡†æµ‹è¯•ã€å®¹é‡è§„åˆ’ã€æ•…éšœæ’æŸ¥

**ğŸš€ ç°åœ¨ä½ æ‹¥æœ‰äº†ä¸€ä¸ªå®Œå…¨ç”Ÿäº§å°±ç»ªçš„ä¼ä¸šçº§Outboxè§£å†³æ–¹æ¡ˆï¼**
